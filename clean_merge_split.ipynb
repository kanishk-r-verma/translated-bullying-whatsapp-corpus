{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0970c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abc2a5",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9a9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_normalise(txt: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to normalize emojis in the given text using information from an emoji information CSV file.\n",
    "\n",
    "    Args:\n",
    "    - txt: A string containing text that may include emojis.\n",
    "\n",
    "    Returns:\n",
    "    - Normalized text with emojis replaced according to the information in the CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = txt.split()\n",
    "    count = 0\n",
    "    try:\n",
    "        emoji_df = pd.read_csv('pre_process/emoji_info.csv')\n",
    "    except:\n",
    "        print(\"ERROR: You need emoji_info.csv if not present already exceute: get_emoj_transcriptions.ipynb\")\n",
    "        \n",
    "    \n",
    "    # Extract unique emoji decimal and emoji values from the DataFrame\n",
    "    emoji_decs = list(emoji_df.emoji_decimal.unique())\n",
    "    emojis = list(emoji_df.emoji.unique())\n",
    "    # Loop through each token in the input tex\n",
    "    \n",
    "    new_tokens = []\n",
    "    for tkn in tokens:\n",
    "        # Check if the token is in the list of emoji decimals\n",
    "        if tkn in emoji_decs:\n",
    "            \n",
    "            emoticon = emoji_df.emoji[emoji_df.emoji_decimal==tkn].values[0]\n",
    "            count += 1\n",
    "            tkn = emoticon\n",
    "        \n",
    "        # Check if the token is in the list of emojis\n",
    "        elif tkn in emojis:\n",
    "            tkn = tkn\n",
    "\n",
    "        else:\n",
    "            tkn = tkn\n",
    "            \n",
    "        new_tokens.append(tkn)\n",
    "    \n",
    "    return ' '.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f580a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_remove(txt: str):\n",
    "    \"\"\"\n",
    "    Function to remove emojis from the given text based on information from an emoji information CSV file.\n",
    "\n",
    "    Args:\n",
    "    - txt: A string containing text that may include emojis.\n",
    "\n",
    "    Returns:\n",
    "    - Text with emojis removed based on the information in the CSV file.\n",
    "    \"\"\"\n",
    "    tokens = txt.split()\n",
    "    count = 0\n",
    "    emoji_df = pd.read_csv('pre_process/emoji_info.csv')\n",
    "    \n",
    "    emoji_decs = list(emoji_df.emoji_decimal.unique())\n",
    "    \n",
    "    emojis = list(emoji_df.emoji.unique())\n",
    "    \n",
    "    emoji_pattern = r'&#[0-9]+;[^\\s,;\"]*'\n",
    "    \n",
    "    new_tokens = []\n",
    "    # Loop through each token in the input text\n",
    "    for tkn in tokens:\n",
    "         # Check if the token is in the list of emoji decimals\n",
    "        if tkn in emoji_decs:\n",
    "            count += 1\n",
    "            tkn = \"\"\n",
    "         # Check if the token is in the list of emojis\n",
    "        elif tkn in emojis:\n",
    "            tkn = \"\"\n",
    "            \n",
    "        elif len(re.findall(emoji_pattern, tkn))>=1:\n",
    "            tkn = \"\"\n",
    "        else:\n",
    "            tkn = tkn\n",
    "            \n",
    "        new_tokens.append(tkn)\n",
    "    \n",
    "    return ' '.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122c0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(sent: str):\n",
    "    \"\"\"\n",
    "    Function to extract timestamp from a sentence using a specific date-time pattern.\n",
    "\n",
    "    Args:\n",
    "    - sent: A string containing text that may include a timestamp.\n",
    "\n",
    "    Returns:\n",
    "    - Timestamp extracted from the sentence based on the provided date-time pattern.\n",
    "    \"\"\"\n",
    "    \n",
    "    #  date-time pattern to search for in the sentence\n",
    "    date_time_pattern = '([0-9]+ / [0-9]+ / [0-9]+ , [0-9]+ : [0-9]+)'\n",
    "\n",
    "    dateTime_list = re.findall(date_time_pattern, sent)\n",
    "    \n",
    "     # Check if any date-time patterns were found\n",
    "    if len(dateTime_list)>0:\n",
    "        temp_sent = sent.split(dateTime_list[0])[-1]\n",
    "        tempSent_tokens = temp_sent.split(':')\n",
    "\n",
    "        dateTime = dateTime_list[0]\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        dateTime = 'empty'\n",
    "        \n",
    "    return dateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad5dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_message(sent: str):\n",
    "    \"\"\"\n",
    "    Function to extract date-time pattern, author and message from a sentence.\n",
    "\n",
    "    Args:\n",
    "    - sent: A string containing text that may include author and message information.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing the extracted author and message from the sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    date_time_pattern = '([0-9]+ / [0-9]+ / [0-9]+ , [0-9]+ : [0-9]+)'\n",
    "\n",
    "    dateTime_list = re.findall(date_time_pattern, sent)\n",
    "    \n",
    "    if len(dateTime_list)>0:\n",
    "        temp_sent = sent.split(dateTime_list[0])[-1]\n",
    "        tempSent_tokens = temp_sent.split(':')\n",
    "\n",
    "        dateTime = dateTime_list[0]\n",
    "        \n",
    "        # Check the structure of the remaining tokens after splitting by ':'\n",
    "        if len(tempSent_tokens)>2:\n",
    "            \n",
    "            auth = tempSent_tokens[0]\n",
    "            msg = ' '.join(tempSent_tokens[1:])\n",
    "            \n",
    "        else:\n",
    "            auth = tempSent_tokens[0]\n",
    "            msg = tempSent_tokens[-1]\n",
    "            \n",
    "    else:\n",
    "        # If no date-time pattern is found, set dateTime as 'empty'\n",
    "        dateTime = 'empty'\n",
    "        tempSent_tokens = sent.split(':')\n",
    "        \n",
    "        if (len(tempSent_tokens))>2:\n",
    "            auth = tempSent_tokens[0]\n",
    "            msg = ' '.join(tempSent_tokens[1:])\n",
    "\n",
    "        elif (len(tempSent_tokens)<2):\n",
    "            # Handle specific cases where the structure is different\n",
    "            if 'jpg' not in tempSent_tokens[0]:\n",
    "                temp_2 = tempSent_tokens[0].split('  ')\n",
    "                auth = temp_2[0]\n",
    "                msg = temp_2[-1]\n",
    "\n",
    "            else:\n",
    "\n",
    "                auth = 'empty'\n",
    "                msg = tempSent_tokens[0]\n",
    "\n",
    "        else:\n",
    "            auth = tempSent_tokens[0]\n",
    "            msg = tempSent_tokens[-1]\n",
    "            \n",
    "    \n",
    "    return auth, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1858122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df: pd.DataFrame):\n",
    "\n",
    "    it_data = []\n",
    "    en_data = []\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "\n",
    "        file = df.File.iloc[i]\n",
    "        sn = df.SentenceNumber.iloc[i]\n",
    "\n",
    "        sent = df.Sentence.iloc[i]\n",
    "\n",
    "        trans = df.Translation.iloc[i]\n",
    "\n",
    "        it_auth, it_msg = author_message(sent)\n",
    "        en_auth, en_msg = author_message(trans)\n",
    "\n",
    "        it_timeStamp = get_timestamp(sent)\n",
    "        en_timeStamp = get_timestamp(trans)\n",
    "\n",
    "        it_emoji_msg = emoji_normalise(it_msg)\n",
    "        it_emoji_auth = emoji_normalise(it_auth)\n",
    "\n",
    "        en_emoji_msg = emoji_normalise(en_msg)\n",
    "        en_emoji_auth = emoji_normalise(en_auth)\n",
    "\n",
    "        it_non_msg = emoji_remove(it_msg)\n",
    "        it_non_auth = emoji_remove(it_auth)\n",
    "\n",
    "        en_non_msg = emoji_remove(en_msg)\n",
    "        en_non_auth = emoji_remove(en_auth)\n",
    "\n",
    "\n",
    "        it_tup = (file, sn, sent, trans, it_timeStamp, it_auth, it_msg, it_emoji_auth, it_emoji_msg, it_non_auth, it_non_msg)\n",
    "        en_tup = (file, sn, sent, trans, en_timeStamp, en_auth, en_msg, en_emoji_auth, en_emoji_msg, en_non_auth, en_non_msg)\n",
    "\n",
    "        it_data.append(it_tup)\n",
    "        en_data.append(en_tup)\n",
    "        \n",
    "    \n",
    "    it_df = pd.DataFrame(it_data, columns=[\"File\", \"SentenceNumber\", \"Sentence\", \"Translation\",\n",
    "                                           \"TimeStamp\", \"author\", \"raw\", \"authorEmoji\", \"textEmoji\",\n",
    "                                          \"authorNon\", \"textNon\"])\n",
    "    \n",
    "    en_df = pd.DataFrame(en_data, columns=[\"File\", \"SentenceNumber\", \"Sentence\", \"Translation\",\n",
    "                                           \"TimeStamp\", \"author\", \"raw\", \"authorEmoji\", \"textEmoji\",\n",
    "                                          \"authorNon\", \"textNon\"])\n",
    "\n",
    "    return it_df, en_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd1cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary(bull: pd.DataFrame):\n",
    "\n",
    "    new_data = []\n",
    "\n",
    "    for f in bull.File.unique():\n",
    "        snums = bull.SentenceNumber[bull.File==f].unique()\n",
    "        for sn in snums:\n",
    "            it_text = bull.Sentence[(bull.SentenceNumber==sn)&(bull.File==f)].values.tolist()[0]\n",
    "            en_text = bull.Translation[(bull.SentenceNumber==sn)&(bull.File==f)].values.tolist()[0]\n",
    "\n",
    "            ent = bull.EntityType[(bull.SentenceNumber==sn)&(bull.File==f)].values.tolist()\n",
    "            rol = bull.Role[(bull.SentenceNumber==sn)&(bull.File==f)].values.tolist()\n",
    "\n",
    "            sar = bull.Sarcasm[(bull.SentenceNumber==sn)&(bull.File==f)].values.tolist()[0]\n",
    "            non = bull.NonOffensive[(bull.SentenceNumber==sn)&(bull.File==f)].values.tolist()[0]\n",
    "\n",
    "            if (ent in [\"Defense\", \"Encouragement_to_the_Harasser\"]) and (rol == 'Victim'):\n",
    "                lab = 0\n",
    "            else:\n",
    "                lab = 1\n",
    "\n",
    "            for e in ent:\n",
    "                for r in rol:\n",
    "                    tup = (f, sn, it_text, en_text, e, r, sar, non, lab)\n",
    "                    new_data.append(tup)\n",
    "                    \n",
    "    new_data = list(set(new_data))\n",
    "\n",
    "    new_df = pd.DataFrame(new_data, columns=[\"File\", \"SentenceNumber\", \"Sentence\", \n",
    "                                             \"Translation\", \"EntityType\", \"Role\",\n",
    "                                             \"Sarcasm\", \"NonOffensive\", \"Binary\"])\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45225512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_frames(bull: pd.DataFrame, data: pd.DataFrame):\n",
    "    \n",
    "    \n",
    "    temp = data.merge(bull, on=([\"File\", \"SentenceNumber\", \"Sentence\", \"Translation\"]),\n",
    "                      how=\"left\")\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "203711ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_merge_mt(it_df, en_df):\n",
    "    \n",
    "    merged_list = []\n",
    "    for i in range(len(it_df)):\n",
    "        mt_w = it_df.mt_w.iloc[i]\n",
    "        mt_wot = it_df.mt_wot.iloc[i]\n",
    "        mt_w_b = it_df.mt_w_b[i]\n",
    "        mt_wot_b = it_df.mt_wot_b[i]\n",
    "        \n",
    "        file = it_df.File.iloc[i]\n",
    "        snum = it_df.SentenceNumber.iloc[i]\n",
    "        \n",
    "        it_txt = en_df.Sentence[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        \n",
    "        en_txt = en_df.Translation[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        ts = en_df.TimeStamp[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        auth = en_df.author[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        en_txt_raw = en_df.raw[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        en_auth_emo = en_df.authorEmoji[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        en_txt_emo = en_df.textEmoji[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        en_auth_non = en_df.authorNon[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        en_txt_non = en_df.textNon[(it_df.File==file)&(it_df.SentenceNumber==snum)].values[0]\n",
    "        \n",
    "        tup = (file, snum, it_txt, en_txt, ts, auth, en_txt_raw, en_auth_emo, en_txt_emo, \n",
    "              en_auth_non, en_txt_non, mt_w, mt_wot, mt_w_b, mt_wot_b)\n",
    "        \n",
    "        merged_list.append(tup)\n",
    "    \n",
    "    merged_df = pd.DataFrame(merged_list, columns=[\"File\", \"SentenceNumber\", \"Sentence\", \n",
    "                                                  \"Translation\", \"TimeStamp\", \"Author\",\n",
    "                                                  \"en_raw\", \"en_auth\", \"en_emo\", \n",
    "                                                  \"en_auth_non\", \"en_non\", \n",
    "                                                  \"mt_w\", \"mt_wot\", \"mt_w_b\", \"mt_wot_b\"])\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b70efb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def it_merge_mt(data_w_b, data_wot_b, data_w, data_wot, df):\n",
    "    \n",
    "    merged_list = []\n",
    "    for i in range(len(data_w)):\n",
    "        mt_w = data_w[i]\n",
    "        mt_wot = data_wot[i]\n",
    "        mt_w_b = data_w_b[i]\n",
    "        mt_wot_b = data_wot_b[i]\n",
    "        \n",
    "        file = df.File.iloc[i]\n",
    "        snum = df.SentenceNumber.iloc[i]\n",
    "        it_txt = df.Sentence.iloc[i]\n",
    "        en_txt = df.Translation.iloc[i]\n",
    "        ts = df.TimeStamp.iloc[i]\n",
    "        auth = df.author.iloc[i]\n",
    "        it_txt_raw = df.raw.iloc[i]\n",
    "        it_auth_emo = df.authorEmoji.iloc[i]\n",
    "        it_txt_emo = df.textEmoji.iloc[i]\n",
    "        it_auth_non = df.authorNon.iloc[i]\n",
    "        it_txt_non = df.textNon.iloc[i]\n",
    "        \n",
    "        tup = (file, snum, it_txt, en_txt, ts, auth, it_txt_raw, it_auth_emo, it_txt_emo, \n",
    "              it_auth_non, it_txt_non, mt_w, mt_wot, mt_w_b, mt_wot_b)\n",
    "        \n",
    "        merged_list.append(tup)\n",
    "    \n",
    "    merged_df = pd.DataFrame(merged_list, columns=[\"File\", \"SentenceNumber\", \"Sentence\", \n",
    "                                                  \"Translation\", \"TimeStamp\", \"Author\",\n",
    "                                                  \"it_raw\", \"it_auth\", \"it_emo\", \n",
    "                                                  \"it_auth_non\", \"it_non\", \n",
    "                                                  \"mt_w\", \"mt_wot\", \"mt_w_b\", \"mt_wot_b\"])\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdfacf",
   "metadata": {},
   "source": [
    "# Prepare Data for MT experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd44000",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all = pd.read_csv(\"merged/merged_allData.csv\")\n",
    "\n",
    "merged_lab = pd.read_csv(\"merged/merged_bullyData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbf24a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji_info.csv          \u001b[31mscenarioC.csv\u001b[m\u001b[m           \u001b[31mtrain.csv\u001b[m\u001b[m\r\n",
      "raw_english_ordered.csv \u001b[34mscenario_wise\u001b[m\u001b[m           \u001b[31mvalidation.csv\u001b[m\u001b[m\r\n",
      "raw_italian_ordered.csv \u001b[31mtest.csv\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls pre_process/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5d619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['merged/D_allData.csv',\n",
       " 'merged/A_allData.csv',\n",
       " 'merged/B_allData.csv',\n",
       " 'merged/B_bullyData.csv',\n",
       " 'merged/A_bullyData.csv',\n",
       " 'merged/D_bullyData.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[f for f in glob(\"merged/*.csv\") if ('merged_' not in f) and ('C_' not in f)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c67c6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_all = pd.read_csv(\"merged/A_allData.csv\")\n",
    "\n",
    "a_bul = pd.read_csv(\"merged/A_bullyData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3782de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1077/1077 [00:47<00:00, 22.57it/s]\n"
     ]
    }
   ],
   "source": [
    "a_it_clean, a_en_clean = process(a_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fddfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_all = pd.read_csv(\"merged/B_allData.csv\")\n",
    "\n",
    "b_bul = pd.read_csv(\"merged/B_bullyData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45cc7fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:27<00:00, 20.82it/s]\n"
     ]
    }
   ],
   "source": [
    "b_it_clean, b_en_clean = process(b_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c4052be",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_all = pd.read_csv(\"merged/C_allData.csv\")\n",
    "\n",
    "c_bul = pd.read_csv(\"merged/C_bullyData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a41506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:07<00:00, 16.88it/s]\n"
     ]
    }
   ],
   "source": [
    "c_it_clean, c_en_clean = process(c_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df624a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_all = pd.read_csv(\"merged/D_allData.csv\")\n",
    "\n",
    "d_bul = pd.read_csv(\"merged/D_bullyData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a119ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:19<00:00, 20.82it/s]\n"
     ]
    }
   ],
   "source": [
    "d_it_clean, d_en_clean = process(d_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c1d342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_merg_all = pd.read_csv(\"merged/merged_allData.csv\")\n",
    "\n",
    "raw_merg_lab = pd.read_csv(\"merged/merged_bullyData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f390ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>SentenceNumber</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Vittima : Ciao . . . volevo invitarvi al saggi...</td>\n",
       "      <td>Vittima : Hi . . . I wanted to invite you to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_1</td>\n",
       "      <td>1</td>\n",
       "      <td>Vittima : WA0001 . jpg ( file allegato )</td>\n",
       "      <td>Vittima : WA0001 . jpg (file attached)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_1</td>\n",
       "      <td>2</td>\n",
       "      <td>‪SupportoVittima1‬ : Chiedo a mia mamma e to f...</td>\n",
       "      <td>‪SupportoVittima1 : I’ll ask my mom and I'll l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_1</td>\n",
       "      <td>3</td>\n",
       "      <td>Vittima : Grazie SupportoVittima1 !</td>\n",
       "      <td>Vittima : Thanks SupportoVittima1!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_1</td>\n",
       "      <td>4</td>\n",
       "      <td>‪Bullo1‬ : Ah un saggio di danza</td>\n",
       "      <td>‪Bullo1 : Ah a dance recital</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File  SentenceNumber                                           Sentence  \\\n",
       "0  A_1               0  Vittima : Ciao . . . volevo invitarvi al saggi...   \n",
       "1  A_1               1           Vittima : WA0001 . jpg ( file allegato )   \n",
       "2  A_1               2  ‪SupportoVittima1‬ : Chiedo a mia mamma e to f...   \n",
       "3  A_1               3                Vittima : Grazie SupportoVittima1 !   \n",
       "4  A_1               4                   ‪Bullo1‬ : Ah un saggio di danza   \n",
       "\n",
       "                                         Translation  \n",
       "0  Vittima : Hi . . . I wanted to invite you to t...  \n",
       "1             Vittima : WA0001 . jpg (file attached)  \n",
       "2  ‪SupportoVittima1 : I’ll ask my mom and I'll l...  \n",
       "3                 Vittima : Thanks SupportoVittima1!  \n",
       "4                       ‪Bullo1 : Ah a dance recital  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_merg_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76d34e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2192/2192 [01:41<00:00, 21.51it/s]\n"
     ]
    }
   ],
   "source": [
    "rawMerge_it_clean, rawMerge_en_clean = process(raw_merg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58fc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawMerge_en_clean.to_csv(\"pre_process/raw_english_ordered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56d1f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is MT input\n",
    "\n",
    "rawMerge_it_clean.to_csv(\"pre_process/raw_italian_ordered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2260c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_it_clean.to_csv(\"pre_process/scenario_wise/A_it_clean.csv\", index=False)\n",
    "\n",
    "b_it_clean.to_csv(\"pre_process/scenario_wise/B_it_clean.csv\", index=False)\n",
    "\n",
    "c_it_clean.to_csv(\"pre_process/scenario_wise/C_it_clean.csv\", index=False)\n",
    "\n",
    "d_it_clean.to_csv(\"pre_process/scenario_wise/D_it_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f183f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_en_clean.to_csv(\"pre_process/scenario_wise/A_en_clean.csv\", index=False)\n",
    "\n",
    "b_en_clean.to_csv(\"pre_process/scenario_wise/B_en_clean.csv\", index=False)\n",
    "\n",
    "c_en_clean.to_csv(\"pre_process/scenario_wise/C_en_clean.csv\", index=False)\n",
    "\n",
    "d_en_clean.to_csv(\"pre_process/scenario_wise/D_en_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2c7a2",
   "metadata": {},
   "source": [
    "# MT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39922fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_italian_ordered was shared for MT experiments, now we will combine all mt-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a462edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "only it-en uses emojis\n",
    "Test-better system is \n",
    "'''\n",
    "\n",
    "with open(\"mt_output/it-en.test_better.hyp.v2\", \"r\") as file:\n",
    "    data_w_b = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98b9f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "only it1-en does not use any emojis as they are replaced with \" \" (blank space)\n",
    "Test is not normal system\n",
    "'''\n",
    "\n",
    "with open(\"mt_output/it-en.test.hyp.v2\", \"r\") as file:\n",
    "    data_w = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a8de887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"mt_output/it1-en.test.hyp.v2\", \"r\") as file:\n",
    "    data_wot = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4da44b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mt_output/it1-en.test_better.hyp.v2\", \"r\") as file:\n",
    "    data_wot_b = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dc16051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2192, 2192, 2192, 2192)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_w), len(data_wot), len(data_w_b), len(data_wot_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "405d7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_merged_all = it_merge_mt(data_w_b, data_wot_b, data_w, data_wot, rawMerge_it_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d371842",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_merged_all = en_merge_mt(it_merged_all, rawMerge_en_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cc07f",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8f629b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(df):\n",
    "    train_val, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train, val = train_test_split(train_val, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb234212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ac31c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_but_c = en_merged_all[['File', 'SentenceNumber']][~(en_merged_all.File.str.contains('C'))]\n",
    "\n",
    "only_c = en_merged_all[['File', 'SentenceNumber']][(en_merged_all.File.str.contains('C'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bbce4081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2062, 130)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_but_c), len(only_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8a5a6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = get_split(all_but_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ed8778f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_val_test/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b3a7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv(\"train_val_test/validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9f9d40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"train_val_test/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6134de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_c.to_csv(\"train_val_test/scenario_c.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a518446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
