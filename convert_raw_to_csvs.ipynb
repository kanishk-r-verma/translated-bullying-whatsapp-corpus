{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fe3b71",
   "metadata": {},
   "source": [
    "# Importing Project Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab63347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter #https://docs.python.org/3/library/collections.html\n",
    "import pandas as pd #https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "\n",
    "import json #https://docs.python.org/3/library/json.html\n",
    "\n",
    "from glob import glob #https://docs.python.org/3/library/glob.html\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup #https://pypi.org/project/beautifulsoup4/\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597da93",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cc3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_data(path: str):\n",
    "    \n",
    "    '''\n",
    "    Extracts the xml data into two parts one with only token-info and \n",
    "    the other with token and label-info for annotated data.\n",
    "    \n",
    "    Args:\n",
    "    @param: path - Path to the XML file\n",
    "    \n",
    "    Returns: token_info - List of token elements from the XML\n",
    "             bull_info - List of BULLISM_INSTANCE elements from the XML\n",
    "    '''\n",
    "    \n",
    "    # Open the XML file\n",
    "    with open(path, \"r\") as xml_file:\n",
    "        xml_content = xml_file.read() # Read the content of the file\n",
    "        \n",
    "    xml_data = BeautifulSoup(xml_content, \"xml\") # Parse XML content https://tedboy.github.io/bs4_doc/_modules/bs4.html#BeautifulSoup\n",
    "    \n",
    "    token_info = xml_data.find_all(\"token\") # Find elements with tag https://tedboy.github.io/bs4_doc/generated/generated/bs4.BeautifulSoup.find_all.html#bs4.BeautifulSoup.find_all\n",
    "    \n",
    "    bull_info = xml_data.find_all(\"BULLISM_INSTANCE\") # Find elements with labels\n",
    "    \n",
    "    # Return the extracted token and BULLISM_INSTANCE elements\n",
    "    \n",
    "    \n",
    "    return token_info, bull_info \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe19687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(data: dict):\n",
    "    \"\"\"\n",
    "    Sorts a dictionary of dictionaries by both sentence numbers (outer keys) and token numbers (inner keys).\n",
    "\n",
    "    Args:\n",
    "    - data (dict): Dictionary of dictionaries to be sorted\n",
    "\n",
    "    Returns:\n",
    "    - sorted_data (dict): Sorted dictionary of dictionaries\n",
    "    \"\"\"\n",
    "    sorted_data = {}\n",
    "    sorted_sentence_nums = sorted(data.keys(), key=lambda x: int(x))\n",
    "    for sentence_num in sorted_sentence_nums:\n",
    "        token_dict = data[sentence_num]\n",
    "        sorted_tokens = sorted(token_dict.keys(), key=lambda x: int(x))\n",
    "        sorted_token_dict = {token_num: token_dict[token_num] for token_num in sorted_tokens}\n",
    "        sorted_data[sentence_num] = sorted_token_dict\n",
    "    return sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b58138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_dict(orig_files: list):\n",
    "    '''\n",
    "    Comments: Generated by Codegen2 (GPT-like) \n",
    "    \n",
    "    The `xml_to_dict` function processes a list of file paths containing XML files \n",
    "    to extract token and label information. \n",
    "\n",
    "    Input:\n",
    "        - orig_files: List of file paths to XML files\n",
    "    \n",
    "    Returns:\n",
    "        - file_text_data: Dictionary containing extracted text data per file\n",
    "        - file_label_data: Dictionary containing extracted label data per file\n",
    "\n",
    "    Processing Steps:\n",
    "        1. Initializes dictionaries to store text and label data per file.\n",
    "        2. Iterates through each file path in the input list.\n",
    "        3. Extracts the file name from the path and displays a progress message.\n",
    "        4. Retrieves token and BULLISM_INSTANCE elements from the XML file using `get_xml_data`.\n",
    "        5. Stores token and label data into separate lists.\n",
    "        6. Extracts unique sentence numbers from the collected data.\n",
    "        7. Organizes text data into a nested dictionary by sentence number and token info.\n",
    "        8. Sorts the text data dictionary.\n",
    "        9. Organizes label data into a nested dictionary by sentence number and token info.\n",
    "        10. Sorts the label data dictionary.\n",
    "        11. Stores sorted text and label data in respective dictionaries for the file.\n",
    "        12. Displays completion messages for each file processed.\n",
    "        13. Returns dictionaries containing extracted text and label data per file.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    text_dict = {}\n",
    "    label_dict = {}\n",
    "    \n",
    "    for file in [o_fi for o_fi in orig_files]:\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        # Extract file name from path\n",
    "        fname = file.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        print(\"Processing\\t:\", fname)\n",
    "        \n",
    "        # Extract token and BULLISM_INSTANCE elements from the XML file\n",
    "        token_info, bull_info = get_xml_data(file)\n",
    "        \n",
    "        print(\"XML to dict converted...\")\n",
    "        print('Now getting tokens and labels...')\n",
    "        \n",
    "        label_data = []\n",
    "        \n",
    "        text_data = []\n",
    "        for i in range(len(token_info)):\n",
    "            t_id = token_info[i].get(\"t_id\")\n",
    "            token = token_info[i].text\n",
    "            s_n = token_info[i].get(\"sentence\")\n",
    "            \n",
    "            text_data.append((s_n, t_id, token))\n",
    "            \n",
    "            # Loop through bull_info\n",
    "            for j in range(len(bull_info)):\n",
    "                token_anchor = bull_info[j].find_all(\"token_anchor\")\n",
    "                anchor_list = [anchor.get(\"t_id\") for anchor in token_anchor]\n",
    "                \n",
    "                # Check if t_id is in anchor_list\n",
    "                if t_id in anchor_list:\n",
    "                    \n",
    "                    label_data.append((s_n, t_id, token))\n",
    "            \n",
    "        # Extract unique sentence numbers from text and label data\n",
    "        txt_unq_sent_num = list(set([txt[0] for txt in text_data]))\n",
    "        lab_unq_sent_num = list(set([label[0] for label in label_data]))\n",
    "        \n",
    "        # Organize text data into a nested dictionary by sentence number and token info\n",
    "        text_data_dict = {}\n",
    "        for sent_num in txt_unq_sent_num:\n",
    "            token_dict = {}\n",
    "            for tup in text_data:\n",
    "                if tup[0] == sent_num:\n",
    "                    token_dict[tup[1]] = tup[-1]\n",
    "            \n",
    "            text_data_dict[sent_num] = token_dict\n",
    "            \n",
    "        # Sort the text data dictionary\n",
    "        sort_text_dict = sort_data(text_data_dict)\n",
    "                    \n",
    "        # Organize label data into a nested dictionary by sentence number and token info\n",
    "        label_data_dict = {}\n",
    "        for sent_num in lab_unq_sent_num:\n",
    "            token_info_dict = {}\n",
    "            for tup in label_data:\n",
    "                if tup[0] == sent_num:\n",
    "                    token_info_dict[tup[1]] = tup[-1]\n",
    "                    \n",
    "        \n",
    "            label_data_dict[sent_num] = token_info_dict\n",
    "            \n",
    "        # Sort the label data dictionary\n",
    "        sort_label_dict = sort_data(label_data_dict)\n",
    "        \n",
    "        # Store sorted text and label data in respective dictionaries for the file\n",
    "        text_dict[fname] = sort_text_dict\n",
    "        label_dict[fname] = sort_label_dict\n",
    "        \n",
    "        print(\"Done..\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"Fin !\")\n",
    "        \n",
    "    return text_dict, label_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d460f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data: dict, sorted_label_dict: dict):\n",
    "    \"\"\"\n",
    "    Process data and extract relevant information.\n",
    "\n",
    "    Args:\n",
    "    - data: A list of dictionaries containing 'annotation' and 'text' keys.\n",
    "    - sorted_label_dict: A dictionary containing sorted label information.\n",
    "\n",
    "    Returns:\n",
    "    - label_count: Count of labeled data instances.\n",
    "    - non_count: Count of instances without labels.\n",
    "    - labels: List of tuples containing extracted information from labeled data.\n",
    "    \"\"\"\n",
    "    label_count = 0  # Initialize count for labeled instances\n",
    "    non_count = 0  # Initialize count for instances without labels\n",
    "    labels = []  # List to store extracted information from labeled data\n",
    "\n",
    "    for d in data:  # Iterate through each dictionary in the provided data\n",
    "        annot_list = d['annotation']  # Extract annotation list\n",
    "        text = d['text']  # Extract text\n",
    "\n",
    "        if len(annot_list) > 0:  # Check if there are annotations\n",
    "            label_count += 1  # Increment labeled instance count\n",
    "            for ann in annot_list:  # Iterate through annotations\n",
    "                # Extract various annotation details\n",
    "                entity_type = ann['entity-type']\n",
    "                role_type = ann['role-type']\n",
    "                sarcasm = ann['sarcasm']\n",
    "                non_offensive = ann['non-offensive']\n",
    "\n",
    "                span_list = ann['span']  # Extract span list\n",
    "                for span in span_list:  # Iterate through spans\n",
    "                    # Extract details from the span\n",
    "                    fname = span.split(\"/\")[-1].split('.xml')[0]\n",
    "                    t_id = span.split(\"/\")[-1].split('.xml')[-1]\n",
    "\n",
    "                    sent_nums = sorted_label_dict[fname].keys()  # Get sentence numbers\n",
    "                    for sn in sent_nums:  # Iterate through sentence numbers\n",
    "                        if t_id in sorted_label_dict[fname][sn].keys():\n",
    "                            # Create a tuple with extracted information and append to labels\n",
    "                            tup = (fname, sn, t_id, ' '.join(text), entity_type, role_type, sarcasm, non_offensive)\n",
    "                            labels.append(tup)\n",
    "        else:\n",
    "            non_count += 1  # Increment count for instances without labels\n",
    "    \n",
    "    labels = list(set(labels))\n",
    "    \n",
    "    label_df = pd.DataFrame(labels, columns=[\"File\", \"SentenceNumber\", \"Token-Id\", \"Sentence\", \"EntityType\", \"Role\", \"Sarcasm\", \"Non-Offensive\"])\n",
    "    \n",
    "    label_df.SentenceNumber = label_df.SentenceNumber.astype(int)\n",
    "    \n",
    "    fnames = list(label_df.File.unique())\n",
    "    \n",
    "    label_df_2 = process_label_data(label_df, fnames)\n",
    "    \n",
    "    return label_count, non_count, label_df_2  # Return counts and extracted labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e0d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label_data(label_df: pd.DataFrame, fnames: list):\n",
    "    \"\"\"\n",
    "    Process label data and generate a list of tuples.\n",
    "\n",
    "    Args:\n",
    "    - label_df: A Pandas DataFrame containing label data.\n",
    "    - fnames: A list of file names.\n",
    "\n",
    "    Returns:\n",
    "    - label_list2: A list of tuples with processed label information.\n",
    "    \"\"\"\n",
    "    label_list2 = []\n",
    "\n",
    "    for f in fnames:\n",
    "        temp = label_df[label_df.File == f]\n",
    "        sn = temp.SentenceNumber.values.tolist()\n",
    "        for n in sn:\n",
    "            temp2 = temp[temp.SentenceNumber == n]\n",
    "            ent = temp2.EntityType.unique()\n",
    "            rol = temp2.Role.unique()\n",
    "            sar = temp2.Sarcasm.unique()\n",
    "            nof = temp2[\"Non-Offensive\"].unique()\n",
    "            sent = temp2.Sentence.unique()[0]\n",
    "\n",
    "            for e in ent:\n",
    "                for r in rol:\n",
    "                    tup = (f, n, sent, e, r, sar[0], nof[0])\n",
    "                    label_list2.append(tup)\n",
    "                    \n",
    "    #label_list2 = list(set(label_list2))\n",
    "                    \n",
    "    label_df = pd.DataFrame(label_list2, columns=[\"File\", \"SentenceNumber\", \"Sentence\", \"EntityType\", \"Role\", \"Sarcasm\", \"Non-Offensive\"])\n",
    "\n",
    "    label_df = label_df.sort_values([\"File\", \"SentenceNumber\"])\n",
    "    \n",
    "    label_df = label_df.drop_duplicates()\n",
    "    \n",
    "    return label_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26ce69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(nested_dict: dict):\n",
    "    \"\"\"\n",
    "    Convert a nested dictionary to a Pandas DataFrame in CSV-like format.\n",
    "\n",
    "    Args:\n",
    "    - nested_dict: A nested dictionary containing file names, sentence numbers, token IDs, and token values.\n",
    "\n",
    "    Returns:\n",
    "    - df: A Pandas DataFrame with columns: File, Token_ids, SentenceNumber, Sentence\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Step 2: Extract data from the nested dictionary and create rows for the DataFrame\n",
    "    for file_name, sentences in nested_dict.items():\n",
    "        for sent_number, tokens in sentences.items():\n",
    "            for token_id, token_value in tokens.items():\n",
    "                rows.append([file_name, sent_number, token_id, token_value])\n",
    "\n",
    "    # Step 3: Create a DataFrame from the collected rows with specific column names\n",
    "    temp = pd.DataFrame(rows, columns=[\"File\", \"SentenceNumber\", \"Token_ids\", \"Sentence\"])\n",
    "    \n",
    "    \n",
    "    # Step 4: Get unique file names from the DataFrame\n",
    "    fnames = temp.File.unique()\n",
    "\n",
    "    df_list = []  # Initialize a list to store DataFrame rows\n",
    "\n",
    "    # Step 5: Iterate through unique file names and concatenate tokens for each unique file and sentence number combination\n",
    "    for f in fnames:\n",
    "        sent_nums = temp.SentenceNumber[temp.File == f].unique()\n",
    "        for sn in sent_nums:\n",
    "            sentence = ' '.join(temp['Sentence'][(temp.SentenceNumber == sn) & (temp.File == f)].values.tolist())\n",
    "            #t_ids = temp['Token_ids'][(temp.SentenceNumber == sn) & (temp.File == f)].values.tolist()\n",
    "            df_list.append((f, sn, sentence))\n",
    "\n",
    "    # Step 6: Create a new DataFrame from the collected information with specific column names\n",
    "    df = pd.DataFrame(df_list, columns=[\"File\", \"SentenceNumber\", \"Sentence\"])\n",
    "\n",
    "    return df  # Return the final DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95c7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_frames(df: pd.DataFrame):\n",
    "    \n",
    "    df.SentenceNumber = df.SentenceNumber.astype(int)\n",
    "    \n",
    "    a_df = df[df.File.str.contains('A')].sort_values([\"File\", \"SentenceNumber\"])\n",
    "    b_df = df[df.File.str.contains('B')].sort_values([\"File\", \"SentenceNumber\"])\n",
    "    c_df = df[df.File.str.contains('C')].sort_values([\"File\", \"SentenceNumber\"])\n",
    "    d_df = df[df.File.str.contains('D')].sort_values([\"File\", \"SentenceNumber\"])\n",
    "    \n",
    "    \n",
    "    return a_df, b_df, c_df, d_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(df, fname):\n",
    "    df.to_csv(fname, index=False)\n",
    "    return \"saved !\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92158a74",
   "metadata": {},
   "source": [
    "# Get RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c6b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path = \"WhatsApp-Dataset/Annotated_Corpus/\"\n",
    "\n",
    "save_path = \"raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebdebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_files = glob(orig_path+\"*.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66baa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take RAW JSON file created by authors and shared over email\n",
    "\n",
    "with open(\"data_whatsapp.json\", \"r\") as file:\n",
    "    \n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d962a183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing\t: A_4\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: C_2\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: A_1\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: A_3\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: C_1\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: A_2\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: D_1\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: D_2\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: B_1\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Processing\t: B_2\n",
      "XML to dict converted...\n",
      "Now getting tokens and labels...\n",
      "Done..\n",
      "\n",
      "\n",
      "Fin !\n"
     ]
    }
   ],
   "source": [
    "sorted_text_dict, sorted_label_dict = xml_to_dict(orig_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a759e019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_text_dict), len(sorted_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "166a7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count, non_count, label_df = process_data(data, sorted_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b023f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1042, 1150)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_count, non_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07aba023",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = convert_to_dataframe(sorted_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c80423",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df, b_df, c_df, d_df = split_frames(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5383f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lab_df, b_lab_df, c_lab_df, d_lab_df = split_frames(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c78e1842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(a_df, save_path+'A_allData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2d9c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(a_lab_df, save_path+'A_bullyData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcf32267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(b_df, save_path+'B_allData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db8120d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(b_lab_df, save_path+'B_bullyData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f88ba59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(c_df, save_path+'C_allData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "037f4517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(c_lab_df, save_path+'C_bullyData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dcb4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(d_df, save_path+'D_allData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5995b7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved !'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(d_lab_df, save_path+'D_bullyData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb4efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
